1. GitHub repo setup
2. Project Template creation
3. Setup & Requirements installation
4. Logging, Utils & Exeption module
5. Model workflows
6. All components notebook experiment
7. All components modular code implementation
8. Training Pipeline
9. MLFlow (MLOps tool) for experiment tracking and model registration
10. DVC (MLOps tool) for pipeline tracking and implementation
11. Prediction Pipeline & User app creation
12. Docker
13. FInal CI/CD Deployment on AWS

---------------------------------------------------------------------
- Object classification task using Deep Learning
- Kidney CT scan images with normal and tumored kidneys (bright colors present in the kidneys and malformed kidneys)
- kidney/renal tumors are growths in the kidneys that can be benign or cancerous. Most do not cause symptoms and are discovered unexpectedly when you are beign diagnosed and treated for another condition.
--------------------------------------------------------------------
Git commands here
1. open Git Bash -> git clone https repo -> cd to access folder -> code . to open the folder in vscode
2. Create template and run with python template.py
3. commit and push to repo: git add . -> git commit -m "message" -> git push origin main -> refresh github repo on web
4. setup.py code and then create virtual env: conda create -n kidney python=3.8 -y
    - conda activate kidney
    - pip install -r requirements.txt
5. Logging, Exeption & Utils:   ------> these 3 things are needed to start the implementation
    - create Logging module
        - open src -> cnn clf -> __init__.py and code the logger
        - import the created logger (CNN_Classifier/__init__.py) in main.py and check the new folder with running logs
    - Technicals for exeptions
        - to see if i'm getting any exeptions, where\when and the type -> use utils folder
        - code the functions
        - open research\trials.ipynb and set the kernel to kidney python environment and check box exeptions and @ensure_annotations decorator
    - commit utils
6. follow Project workflow on readme! Basically update the CNN_Classifier
    - Data Ingestion
        - Download images -> make 2 folders (normal + tumor) -> zip -> add to google drive -> click on zipped folder -> copy link
          -> give access to anyone (important)
          GDRIVE LINK >>> https://drive.google.com/file/d/1Sm3W06lYtO1f3i6RM0z3iiET1zdBj6AO/view?usp=sharing <<<
          PRFIX FOR DOWNLOAD >>> prefix = "https://drive.google.com/uc?/export=download&id=" <<<
        - config.yaml and define variables for data ingestion
        - params.yaml
        - entity -> create everything inside the 01_data_ingestion and then add
        - src\CNN_Classifier\constants\__init__.py -> code the linking between config.yaml and paraml.yaml
        - ...
        - ...
        - ...
Amazing, now if i wanto to update or change something i can just modify the config.yaml file due to the amazing project structure.


################################
Do the same for every stage of the model development.
###############################

REMEMBER TO RUN THIS BEFORE RUNNING THE PIPELINE (when closing and reopening program or terminal):

export MLFLOW_TRACKING_URI=https://dagshub.com/DavidAlexanderMoe/Kidney-Tumor-Detection-End-To-End.mlflow
export MLFLOW_TRACKING_USERNAME=DavidAlexanderMoe 
export MLFLOW_TRACKING_PASSWORD=...


DVC:
Pipeline tracking with DVC

Write code -> cache: false -> make sure you have the .git file inside the repo
-> open terminal -> dvc init -> delete artifacts to run the pipeline again using dvc
-> comment out evaluation.log_into_mlflow() in 04 stage
-> dvc repro (run the dvc.yaml file to run the pipeline) (it will track everything in the dvc.lock file)
If you retype after running the code dvc repro it will let you select the stages to do!

------------------------------------------------------------
$ dvc repro
Stage 'data_ingestion' didn't change, skipping
Stage 'prepare_base_model' didn't change, skipping
Stage 'training' didn't change, skipping
Stage 'evaluation' didn't change, skipping
Data and pipelines are up to date.
------------------------------------------------------------

If i delete a stage from the artifacts directory/make changes to the code and retype dvc repro it will skip the present stages 
and it will do the missing ones!
If you do not make changes in the code the dvc won't run.


dvc dag is the pipeline graph to check dependencies of the pipeline:

$ dvc dag
+----------------+            +--------------------+ 
| data_ingestion |            | prepare_base_model |
+----------------+*****       +--------------------+
         *             *****             *
         *                  ******       *
         *                        ***    *
         **                        +----------+
           **                      | training |
             ***                   +----------+
                ***             ***
                   **         **
                     **     **
                  +------------+
                  | evaluation |
                  +------------+

####################################
app.py
run the app and then copy http://127.0.0.1:8080 and if you want to retrain the model just add http://127.0.0.1:8080/train
to restart training and pipeline


######################################

DOCKERIZATION &AWS CI/CD Deployment

General info:
1. Build docker image of the source code
2. Push your docker image to ECR (elastic container registry)
3. Launch Your EC2 (virtual machine)
4. Pull Your image from ECR in EC2
5. Lauch your docker image in EC2

1. Login to AWS console.
2. Create IAM user for deployment
- when doing the Deployment comment out the evaluation.log_into_mlflow() function since it is not needed (it was needed only for model development)
- enter IAM service on AWS console -> utenti (user on left side) -> create new -> kidney -> attach policies directly (1. AmazonEC2ContainerRegistryFullAccess
2. AmazonEC2FullAccess) -> next -> create user -> enter user -> credenziali di sicurezza -> chiavi di accesso -> crea chiavi di accesso
-> chiavi per accedere all'account AWS -> CLI -> crea chiave di accesso -> download csv file

3. Create ECR repo to store/save docker image
- go back to aws homepage -> search and click for ECR -> crea repository -> privata, name, create -> copy uri
- Save the URI: 406660890134.dkr.ecr.us-east-1.amazonaws.com/kidney

4. Create EC2 machine (Ubuntu) + 5. Open EC2 and Install docker in EC2 Machine:
- AWS homepage -> ec2 -> launch instance -> name kidney-machine, ubuntu, instance type (DL project --> at least 8gb ram --> t2.large),
create key pair Kidney and assign it -> allow ssh, https, http -> storage size (GiB) 32 gb -> launch instance
- istanze -> refresh and wait till it is running -> click on instance is -> connect -> connect again
-> launches a terminal where we will install all the necessary tools
- terminal commands:
clear
#optional
sudo apt-get update -y
sudo apt-get upgrade
--> yes
--> if strange window appears press enter

#required
# install docker on ec2 machine
curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh
sudo usermod -aG docker ubuntu
newgrp docker
docker --version # to see if docker is running

6. Configure EC2 as self-hosted runner:
-> go to github -> project settings -> actions -> runners -> new self hosted runner -> linux and copy one by one all the commands
in the ec2 machine on aws to connect aws ec2 to my github -> run all commands -> (name of runner group) enter -> (name of runner) self-hosted
-> enter -> enter -> copy all apart from the last one
-> back to github -> runners -> if "idle" then it means it is connectedÃ¹

7. Setup github secrets:
-> github project settings -> secrets and variables -> actions -> new repo secret -> enter this
# name 
AWS_ACCESS_KEY_ID
# secret 
copy the first one from download csv
add secret

# new repo secret
# name
AWS_SECRET_ACCESS_KEY
# secret
copy second from csv

# smae for these
AWS_REGION = us-east-1

# uri is 406660890134.dkr.ecr.us-east-1.amazonaws.com/kidney
# copy till .com
AWS_ECR_LOGIN_URI = 406660890134.dkr.ecr.us-east-1.amazonaws.com
# kidney
ECR_REPOSITORY_NAME = kidney

everything is added and now i'm ready to push. make a change to the code and revert it to push to github
make sure everything is fine. there won't be any container running at first so these 3 lines in .github\workflows\main.yaml are commented.
when depolyed, you need to uncomment these

push to github -> refresh repo and see a yellow icon on top (workflow is running) -> click on actions on top -> click on update and
see the CI/CD pipeline happening. wait wait wait for CDelivery and CDeployment (pull image and launch it into ec2 machine)

when everything is fine, check on app.py the port on which the app is running (8080) -> i need to configure the port -> open aws
-> go instances in ec2 -> click on instance id -> security -> click on the security group code -> edit inbound rules (top right on table)
-> add rule -> custom tcp, port number/intervallo porte: 8080, select 0.0.0.0/0 -> save rules
go back to ec2 -> click on instance id -> copy public IPv4 address -> paste it on chrome and add :port# (:8080)
the app will work also if you close (x) aws website
if you want to train the model add /train to the url and it will start the training

to terminate the instance (since it will charge me money) go to ec2 -> instances -> terminate instance and see that it is shutting down
delete also ecr -> delete iam user kidney -> now the app won't be running no more